{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> DS200A Computer Vision Assignment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>  Part Four: Neural networks </h2>\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a neural network classifier using an architecture of your choosing. This application\n",
    "of deep learning can be done in PyTorch, TensorFlow, or a framework of your choice. This is the\n",
    "industry standard for image classification. Describe your network and assess its performance. To\n",
    "receive extra credit, your neural network classifier must outperform your other methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import skimage\n",
    "from skimage import data\n",
    "from skimage import io\n",
    "import os\n",
    "import cv2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_original = pd.read_hdf(\"data.h5\", \"data\")\n",
    "print(len(data_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = data_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16\n",
    "img_w, img_h = 32, 32\n",
    "num_classes = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(x):\n",
    "    return len(x.shape)\n",
    "data_filtered['shape'] = data_filtered['Pictures'].apply(get_shape)\n",
    "data_filtered = data_filtered[data_filtered['shape']==3]\n",
    "print(len(data_filtered))\n",
    "data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def transpose_tensor(x):\n",
    "#     return x.transpose(2,0,1)\n",
    "# data['Pictures'] = data['Pictures'].apply(transpose_tensor)\n",
    "# print(len(data))\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset processing\n",
    "def load_data(df, img_w, img_h):\n",
    "    print('Reading training images')\n",
    "    img_list, label_list = [], []\n",
    "    for index, data in df.iterrows():\n",
    "        img = data['Pictures']\n",
    "        label = data['Encoding']\n",
    "        #resize each image to the standard img_w * img_h \n",
    "        img = cv2.resize(img, (img_w, img_h), cv2.INTER_LINEAR)\n",
    "        img_list.append(img), label_list.append(label)\n",
    "    img_list, label_list = np.array(img_list, dtype=np.uint8), np.array( label_list, dtype=np.int32 )\n",
    "    img_list = img_list.astype('float32')\n",
    "    label_list = label_list.astype('long')\n",
    "    img_list = img_list / 255\n",
    "    return img_list, label_list\n",
    "\n",
    "data, label = load_data(data_filtered, img_w, img_h)\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "idx = np.arange( data.shape[0] )\n",
    "np.random.shuffle(idx)\n",
    "data, label = data[idx], label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "split = np.int(data.shape[0]* split_ratio)\n",
    "x_train, y_train = data[:split].transpose(0,3,1,2), label[:split].reshape(-1)\n",
    "x_val, y_val =data[split:].transpose(0,3,1,2), label[split:].reshape(-1)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "channel1_mean, channel1_std = x_train[:,0,:,:].mean(), x_train[:,0,:,:].std()\n",
    "channel2_mean, channel2_std = x_train[:,1,:,:].mean(), x_train[:,1,:,:].std()\n",
    "channel3_mean, channel3_std = x_train[:,2,:,:].mean(), x_train[:,2,:,:].std()\n",
    "\n",
    "normalize_data = transforms.Compose([\n",
    "    transforms.Normalize((channel1_mean, channel2_mean, channel3_mean),\n",
    "                          (channel1_std, channel2_std, channel3_std)),\n",
    "])    \n",
    "for i in range(len(x_val)):\n",
    "    x_val[i] = normalize_data(torch.from_numpy(x_val[i]))\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i] = normalize_data(torch.from_numpy(x_train[i]))\n",
    "\n",
    "print(x_train[0])\n",
    "print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image, label):\n",
    "        self.image = image\n",
    "        self.label = label\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # stuff\n",
    "        return (self.image[index], self.label[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(x_train, y_train)\n",
    "val_dataset = ImageDataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(QAlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            3, 64, kernel_size=5, stride=1, padding=2)  # 32x32x3 -> 32x32x64\n",
    "        # self.pool1=nn.MaxPool2d(kernel_size=3, stride=2, padding =1 )# 32x32x64\n",
    "        # -> 16x16x64\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            64, 64, kernel_size=5, stride=1, padding=2)  # 16x16x64 -> 16x16x64\n",
    "        # self.pool2=nn.MaxPool2d(kernel_size=3, stride=2, padding = 1)# 16x16x64\n",
    "        # -> 8x8x64\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 384) # self.fc1 = nn.Linear(64 * 8 * 8, 384)\n",
    "        self.fc2 = nn.Linear(384, 192)\n",
    "        self.fc3 = nn.Linear(192, num_classes)\n",
    "\n",
    "    def squeeze_layers(self, sl=None):\n",
    "        for k in self._modules.keys():\n",
    "            if k in sl:\n",
    "                for param in self._modules[k].parameters():\n",
    "                    param.requires_grad = False\n",
    "                    print(param.requires_grad)\n",
    "\n",
    "    def back(self):\n",
    "        for k in self._modules.keys():\n",
    "            for param in self._modules[k].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(self.bn1(F.relu(self.conv1(x))), 3, 2, 1)\n",
    "        x = F.max_pool2d(self.bn2(F.relu(self.conv2(x))), 3, 2, 1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, K):\n",
    "    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\n",
    "\n",
    "def learning_rate_with_decay(\n",
    "        lr,\n",
    "        batch_size,\n",
    "        batch_denom,\n",
    "        batches_per_epoch,\n",
    "        boundary_epochs,\n",
    "        decay_rates):\n",
    "    initial_learning_rate = lr*1. #* batch_size / batch_denom\n",
    "\n",
    "    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n",
    "    vals = [initial_learning_rate * decay for decay in decay_rates]\n",
    "\n",
    "    def learning_rate_fn(itr):\n",
    "        lt = [itr < b for b in boundaries] + [True]\n",
    "        i = np.argmax(lt)\n",
    "        return vals[i]\n",
    "\n",
    "    return learning_rate_fn\n",
    "\n",
    "def inf_generator(iterable):\n",
    "    \"\"\"Allows training with DataLoaders in a single infinite loop:\n",
    "        for i, (x, y) in enumerate(inf_generator(train_loader)):\n",
    "    \"\"\"\n",
    "    iterator = iterable.__iter__()\n",
    "    while True:\n",
    "        try:\n",
    "            yield iterator.__next__()\n",
    "        except StopIteration:\n",
    "            iterator = iterable.__iter__()\n",
    "            \n",
    "def accuracy(model, dataset_loader):#, args):\n",
    "    total_correct = 0\n",
    "    for x, y in dataset_loader:\n",
    "        #if args.gpu:\n",
    "        #    x = x.cuda()\n",
    "        y = one_hot(np.array(y.numpy()), 20)\n",
    "\n",
    "        target_class = np.argmax(y, axis=1)\n",
    "        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\n",
    "        total_correct += np.sum(predicted_class == target_class)\n",
    "    return total_correct / len(dataset_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QAlexNet(num_classes=20)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = inf_generator(train_loader)\n",
    "batches_per_epoch = len(train_loader)\n",
    "\n",
    "lr_fn = learning_rate_with_decay(\n",
    "    0.01,\n",
    "    batch_size,\n",
    "    batch_denom=128,\n",
    "    batches_per_epoch=batches_per_epoch,\n",
    "    boundary_epochs=[30,60],\n",
    "    decay_rates=[1, 0.1, 0.01]\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.01,\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "iterations = 0\n",
    "epoches, train_losses, val_accuracies = [], [], []\n",
    "\n",
    "train_loss = 0.0\n",
    "\n",
    "print(f'Numer of batches per epoch: {batches_per_epoch}')\n",
    "while iterations < (100 * batches_per_epoch):\n",
    "\n",
    "    print(f'Iteration number: {iterations}')\n",
    "    iterations += 1\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr_fn(iterations)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    x, y = data_gen.__next__()\n",
    "#     if args.gpu:\n",
    "#         x = x.cuda()\n",
    "#         y = y.cuda()\n",
    "    logits = model(x)\n",
    "    loss = criterion(logits, y)\n",
    "    train_loss += loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iterations % batches_per_epoch == 0:\n",
    "        train_loss /= batches_per_epoch\n",
    "        epoch += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_acc = accuracy(model, val_loader)#, args)\n",
    "            train_acc = accuracy(model, train_loader)#, args)\n",
    "            print(\n",
    "                \"Epoch {:04d} | Val Acc {:.4f} | Train Acc {:.4f} | Training Loss {:.4f}\".format(\n",
    "                    iterations // batches_per_epoch,\n",
    "                    val_acc, train_acc,\n",
    "                    train_loss))\n",
    "        epoches.append(epoch)\n",
    "        train_losses.append(train_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        train_loss = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
